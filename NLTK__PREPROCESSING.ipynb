{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e48e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d97cf",
   "metadata": {},
   "source": [
    "## Step-1\n",
    "### Convert text to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9818ac92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am doing a good job here.i can do anyting,today!!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def doLowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "inputStr=\"I am doing a good JOB here.I can do ANyting,Today!!\"\n",
    "doLowercase(inputStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3390b",
   "metadata": {},
   "source": [
    "## Step-2\n",
    "### Remove numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b6bc570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You bought  candies from shop, and  candies are in home.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_num(text):\n",
    "    return re.sub(r'\\d+','',text)\n",
    "inputStr = \"You bought 6 candies from shop, and 4 candies are in home.\"\n",
    "remove_num(inputStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d85878e",
   "metadata": {},
   "source": [
    "### Or convert number to text using inflect library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd6ec8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting inflect\n",
      "  Downloading inflect-5.3.0-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: inflect\n",
      "Successfully installed inflect-5.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fff31f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You bought six candies from shop, and four candies are in home.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inflect\n",
    "q=inflect.engine()\n",
    "\n",
    "def convert_num(text):\n",
    "    temp_string=text.split()\n",
    "    \n",
    "    new_str=[]\n",
    "    \n",
    "    for word in temp_string:\n",
    "        if word.isdigit():\n",
    "            temp=q.number_to_words(word)\n",
    "            new_str.append(temp)\n",
    "        else:\n",
    "            new_str.append(word)\n",
    "    temp_str=' '.join(new_str)\n",
    "    return temp_str\n",
    "input_str = 'You bought 6 candies from shop, and 4 candies are in home.'\n",
    "convert_num(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13b790c",
   "metadata": {},
   "source": [
    "## Step-3 \n",
    "### Remove Punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba384928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey Are you excited After a week we will be in Shimla'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rem_punc(text):\n",
    "    translator=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(translator)\n",
    "input_str = \"Hey, Are you excited??, After a week, we will be in Shimla!!!\"\n",
    "rem_punc(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ac01a",
   "metadata": {},
   "source": [
    "## Step-4\n",
    "### Remove Stop words\n",
    "\n",
    "Words that conributes a negligible probabiliy to the context eg. is, the ,a etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4d48b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data', 'new', 'oil', '.', 'A.I', 'last', 'invention']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def rem_stop_words(text):\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    word_tokens=word_tokenize(text)\n",
    "    filtered_text=[word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "input_text = \"Data is the new oil. A.I is the last invention\"\n",
    "rem_stop_words(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc6a4ae",
   "metadata": {},
   "source": [
    "## Step-4\n",
    "### Stemming\n",
    "From Stemming we will process of getting the root form of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "983492a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'is',\n",
       " 'the',\n",
       " 'new',\n",
       " 'revolut',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'day',\n",
       " 'one',\n",
       " 'individu',\n",
       " 'would',\n",
       " 'gener',\n",
       " 'terabyt',\n",
       " 'of',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stem1=PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    words_tokens=word_tokenize(text)\n",
    "    stems=[stem1.stem(word) for word in words_tokens]\n",
    "    return stems\n",
    "\n",
    "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf2150f",
   "metadata": {},
   "source": [
    "## Step-5\n",
    "### Lemmatization\n",
    "As stemming, lemmatization do the same but the only difference is that lemmatization ensures that root word belongs to the language. Because of the use of lemmatization we will get the valid words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01c1d0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'be',\n",
       " 'the',\n",
       " 'newly',\n",
       " 'revolution',\n",
       " 'in',\n",
       " 'the',\n",
       " 'World',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'days',\n",
       " 'one',\n",
       " 'individual',\n",
       " 'would',\n",
       " 'generate',\n",
       " 'terabytes',\n",
       " 'of',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemma=wordnet.WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    word_tokens=word_tokenize(text)\n",
    "    lemmas=[lemma.lemmatize(word,pos='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "text = 'Data is the newly revolution in the World, in a days one individual would generate terabytes of data.'\n",
    "lemmatize_word(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc17fb",
   "metadata": {},
   "source": [
    "## Step-6\n",
    "### Parts of Speech (POS) Tagging\n",
    "The pos(parts of speech) explain you how a word is used in a sentence. In the sentence, a word have different contexts and semantic meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c91d678b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Are', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('afraid', 'IN'),\n",
       " ('of', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def pos_tagg(text):\n",
    "    word_tokens = word_tokenize(text) \n",
    "    return pos_tag(word_tokens)\n",
    "pos_tagg('Are you afraid of something?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab0fde",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "Chunking is the process of extracting phrases from the Unstructured text and give them more structure to it. We also called them shallow parsing.We can do it on top of pos tagging. It groups words into chunks mainly for noun phrases. chunking we do by using regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "091b3201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ red/JJ parrot/NN)\n",
      "  is/VBZ\n",
      "  flying/VBG\n",
      "  in/IN\n",
      "  (NP the/DT sky/NN))\n",
      "(NP the/DT little/JJ red/JJ parrot/NN)\n",
      "(NP the/DT sky/NN)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk import pos_tag \n",
    "\n",
    "def chunking(text, grammar): \n",
    "    word_tokens = word_tokenize(text) \n",
    "  \n",
    "    # label words with pos \n",
    "    word_pos = pos_tag(word_tokens) \n",
    "  \n",
    "    # create chunk parser using grammar \n",
    "    chunkParser = nltk.RegexpParser(grammar) \n",
    "  \n",
    "    # test it on the list of word tokens with tagged pos \n",
    "    tree = chunkParser.parse(word_pos) \n",
    "      \n",
    "    for subtree in tree.subtrees(): \n",
    "        print(subtree) \n",
    "    #tree.draw() \n",
    "      \n",
    "sentence = 'the little red parrot is flying in the sky'\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunking(sentence, grammar) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a0fde",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "It is used to extract information from unstructured text. It is used to classy the entities which is present in the text into categories like a person, organization, event, places, etc. This will give you a detail knowledge about the text and the relationship between the different entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04eaec87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Brain/NNP)\n",
      "  (PERSON Lara/NNP)\n",
      "  scored/VBD\n",
      "  the/DT\n",
      "  highest/JJS\n",
      "  400/CD\n",
      "  runs/NNS\n",
      "  in/IN\n",
      "  a/DT\n",
      "  test/NN\n",
      "  match/NN\n",
      "  which/WDT\n",
      "  played/VBD\n",
      "  in/IN\n",
      "  between/IN\n",
      "  (ORGANIZATION WI/NNP)\n",
      "  and/CC\n",
      "  (GPE England/NNP)\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag, ne_chunk \n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "  \n",
    "def ner(text): \n",
    "    # tokenize the text \n",
    "    word_tokens = word_tokenize(text) \n",
    "  \n",
    "    # pos tagging of words \n",
    "    word_pos = pos_tag(word_tokens) \n",
    "  \n",
    "    # tree of word entities \n",
    "    print(ne_chunk(word_pos)) \n",
    "  \n",
    "text = 'Brain Lara scored the highest 400 runs in a test match which played in between WI and England.'\n",
    "ner(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa99e1",
   "metadata": {},
   "source": [
    "## Word Count\n",
    "\n",
    ".By word frequency we can find out how many times each tokens appear in the text. When talking about word frequency, we distinguished between types and tokens.Types are the distinct words in a corpus, whereas tokens are the words, including repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73906e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.regexp import WhitespaceTokenizer\n",
    "m = \"'There is no need to panic. We need to work together, take small yet importat measures to ensure self-protection,' the Prime Minister tweeted.\"\n",
    "tokens = WhitespaceTokenizer().tokenize(m)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dda84d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "my_vocab = set(tokens)\n",
    "print(len(my_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb98a363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "my_st = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\"\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer\n",
    "m_t = WordPunctTokenizer().tokenize(my_st)\n",
    "\n",
    "print(len(m_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d19ad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "my_vocab = set(tokens)\n",
    "print(len(my_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ad6dda",
   "metadata": {},
   "source": [
    "## Frequency distribution\n",
    "\n",
    "What is Frequency distribution? This is basically counting words in your texts.To give a brief example of how it works,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "690b5e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "<FreqDist with 23 samples and 28 outcomes>\n"
     ]
    }
   ],
   "source": [
    "#from nltk.book import *\n",
    "import nltk\n",
    "#nltk.download('gutenberg')\n",
    "print(\"\\n\\n\\n\")\n",
    "text1 = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\"\n",
    "freqDist = nltk.FreqDist(word_tokenize(text1))\n",
    "print(freqDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3395e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(freqDist[\"person\"])\n",
    "print(freqDist[\"need\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e25747f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
